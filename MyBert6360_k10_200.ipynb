{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m109103/bert/blob/main/MyBert6360_k10_200.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1lnuesyxCqE",
        "outputId": "73749143-4d28-4f98-bc16-8c21c7b4cf85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-bert\n",
            "  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-bert) (1.21.6)\n",
            "Collecting keras-transformer==0.40.0\n",
            "  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
            "Collecting keras-pos-embd==0.13.0\n",
            "  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
            "Collecting keras-multi-head==0.29.0\n",
            "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
            "Collecting keras-layer-normalization==0.16.0\n",
            "  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
            "Collecting keras-position-wise-feed-forward==0.8.0\n",
            "  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
            "Collecting keras-embed-sim==0.10.0\n",
            "  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
            "Collecting keras-self-attention==0.51.0\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33517 sha256=5c7f8f519c2ca81f745692c27868841f396d26450150bbdea1dd9635bb0eb4cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/e8/45/842b3a39831261aef9154b907eacbc4ac99499a99ae829b06f\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12305 sha256=ea06d6c1d4860113524a159b0e39f1e2dc00334064c6b21875520396c0c8b60d\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/68/26/692ed21edd832833c3b0a0e21615bcacd99ca458b3f9ed571f\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3960 sha256=3cb86cc9dca6627ddc303a47cdc202be47b4a6602f40e857f7b507d1d132f47b\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/67/b5/d847588d075895281e1cf5590f819bd4cf076a554872268bd5\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4668 sha256=50396334b5012487efa59550f20c9d994f3d6f0c9bc8969b5ddc417ecb3ca681\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/5d/1c/2e619f594f69fbcf8bc20943b27d414871c409be053994813e\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14993 sha256=9720b82e1f33a46a06c01be09050d5813df842917b9238059ac50e21ea29628c\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/aa/3c/9d15d24005179dae08ff291ce99c754b296347817d076fd9fb\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6962 sha256=9501db01e38ac616a590ba5a2bae261a7b2e25101286e1d84e62233f0cbb5ae9\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/c1/a0/dc44fcf68c857b7ff6be9a97e675e5adf51022eff1169b042f\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4983 sha256=e4a3acf744bff2abf68023753bd8139dde2af69fc5ceb36d4585989b7ed92286\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/75/6f/d42f6e051506f442daeba53ff1e2d21a5f20ef8c411610f2bb\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=08ab7ac2af0867e33e70304027a8ea1c9b8d581971e2ac20cfad23a973bfe51f\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
            "Successfully built keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention\n",
            "Installing collected packages: keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.89.0 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCTpza-0LRzB",
        "outputId": "3a354e60-b27d-4631-9e4c-38080d87d51b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y0Vg7WwLkZN",
        "outputId": "72e17e1e-15c6-447d-866a-c4797b277c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "1.4615392880000115\n",
            "GPU (s):\n",
            "0.037175576999999294\n",
            "GPU speedup over CPU: 39x\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQChBWn7C2oP",
        "outputId": "70bae24c-fd07-462c-c5aa-a38b4c91c8a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/Colab Notebooks/bert6360_200\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive  \n",
        "drive.mount('/content/gdrive')  \n",
        "%cd /content/gdrive/\"My Drive\"/\"Colab Notebooks\"/\"bert6360_200\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PZOcUU4i0IsC"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from keras_bert import (\n",
        "    load_vocabulary,\n",
        "    load_trained_model_from_checkpoint,\n",
        "    Tokenizer,\n",
        "    get_checkpoint_paths,\n",
        ")\n",
        "from keras_bert.datasets import get_pretrained, PretrainedList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujxw-JrC0OHS"
      },
      "outputs": [],
      "source": [
        "# 指定訓練資料與測試資料的比例\n",
        "BATCH_SIZE = 10 # batch size建議不要設得太大，不然很有可能out of memory\n",
        "EPOCHS = 5 # epoch 5次其實就很夠了，當然你可以嘗試再大一點，只是訓練要更久"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW01f8ZM0ZyT",
        "outputId": "23fe66c9-8290-42b9-fdf0-b15175c3c5af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n",
            "381894656/381892918 [==============================] - 3s 0us/step\n",
            "381902848/381892918 [==============================] - 3s 0us/step\n",
            "Model: \"model_1\"\n",
            "________________________________________________________________________________________________________________________\n",
            " Layer (type)                          Output Shape               Param #       Connected to                            \n",
            "========================================================================================================================\n",
            " Input-Token (InputLayer)              [(None, None)]             0             []                                      \n",
            "                                                                                                                        \n",
            " Input-Segment (InputLayer)            [(None, None)]             0             []                                      \n",
            "                                                                                                                        \n",
            " Embedding-Token (TokenEmbedding)      [(None, None, 768),        16226304      ['Input-Token[0][0]']                   \n",
            "                                        (21128, 768)]                                                                   \n",
            "                                                                                                                        \n",
            " Embedding-Segment (Embedding)         (None, None, 768)          1536          ['Input-Segment[0][0]']                 \n",
            "                                                                                                                        \n",
            " Embedding-Token-Segment (Add)         (None, None, 768)          0             ['Embedding-Token[0][0]',               \n",
            "                                                                                 'Embedding-Segment[0][0]']             \n",
            "                                                                                                                        \n",
            " Embedding-Position (PositionEmbedding  (None, None, 768)         393216        ['Embedding-Token-Segment[0][0]']       \n",
            " )                                                                                                                      \n",
            "                                                                                                                        \n",
            " Embedding-Dropout (Dropout)           (None, None, 768)          0             ['Embedding-Position[0][0]']            \n",
            "                                                                                                                        \n",
            " Embedding-Norm (LayerNormalization)   (None, None, 768)          1536          ['Embedding-Dropout[0][0]']             \n",
            "                                                                                                                        \n",
            " Encoder-1-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Embedding-Norm[0][0]']                \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-1-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-1-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-1-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Embedding-Norm[0][0]',                \n",
            " (Add)                                                                           'Encoder-1-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-1-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-1-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-1-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-1-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-1-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-1-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-1-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-1-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-1-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-1-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-1-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-2-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-1-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-2-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-2-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-2-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-1-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-2-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-2-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-2-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-2-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-2-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-2-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-2-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-2-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-2-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-2-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-2-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-2-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-3-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-2-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-3-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-3-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-3-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-2-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-3-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-3-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-3-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-3-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-3-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-3-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-3-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-3-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-3-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-3-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-3-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-3-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-4-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-3-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-4-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-4-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-4-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-3-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-4-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-4-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-4-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-4-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-4-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-4-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-4-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-4-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-4-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-4-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-4-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-4-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-5-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-4-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-5-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-5-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-5-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-4-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-5-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-5-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-5-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-5-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-5-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-5-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-5-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-5-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-5-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-5-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-5-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-5-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-6-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-5-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-6-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-6-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-6-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-5-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-6-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-6-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-6-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-6-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-6-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-6-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-6-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-6-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-6-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-6-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-6-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-6-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-7-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-6-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-7-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-7-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-7-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-6-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-7-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-7-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-7-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-7-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-7-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-7-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-7-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-7-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-7-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-7-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-7-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-7-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-8-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-7-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-8-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-8-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-8-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-7-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-8-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-8-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-8-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-8-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-8-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-8-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-8-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-8-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-8-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-8-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-8-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-8-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-9-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-8-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-9-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-9-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-9-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-8-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-9-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-9-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-9-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-9-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-9-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-9-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-9-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-9-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-9-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-9-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-9-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-9-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-10-MultiHeadSelfAttention (Mu  (None, None, 768)         2362368       ['Encoder-9-FeedForward-Norm[0][0]']    \n",
            " ltiHeadAttention)                                                                                                      \n",
            "                                                                                                                        \n",
            " Encoder-10-MultiHeadSelfAttention-Dro  (None, None, 768)         0             ['Encoder-10-MultiHeadSelfAttention[0][0\n",
            " pout (Dropout)                                                                 ]']                                     \n",
            "                                                                                                                        \n",
            " Encoder-10-MultiHeadSelfAttention-Add  (None, None, 768)         0             ['Encoder-9-FeedForward-Norm[0][0]',    \n",
            "  (Add)                                                                          'Encoder-10-MultiHeadSelfAttention-Drop\n",
            "                                                                                out[0][0]']                             \n",
            "                                                                                                                        \n",
            " Encoder-10-MultiHeadSelfAttention-Nor  (None, None, 768)         1536          ['Encoder-10-MultiHeadSelfAttention-Add[\n",
            " m (LayerNormalization)                                                         0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-10-FeedForward (FeedForward)  (None, None, 768)          4722432       ['Encoder-10-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]']                                \n",
            "                                                                                                                        \n",
            " Encoder-10-FeedForward-Dropout (Dropo  (None, None, 768)         0             ['Encoder-10-FeedForward[0][0]']        \n",
            " ut)                                                                                                                    \n",
            "                                                                                                                        \n",
            " Encoder-10-FeedForward-Add (Add)      (None, None, 768)          0             ['Encoder-10-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]',                                \n",
            "                                                                                 'Encoder-10-FeedForward-Dropout[0][0]']\n",
            "                                                                                                                        \n",
            " Encoder-10-FeedForward-Norm (LayerNor  (None, None, 768)         1536          ['Encoder-10-FeedForward-Add[0][0]']    \n",
            " malization)                                                                                                            \n",
            "                                                                                                                        \n",
            " Encoder-11-MultiHeadSelfAttention (Mu  (None, None, 768)         2362368       ['Encoder-10-FeedForward-Norm[0][0]']   \n",
            " ltiHeadAttention)                                                                                                      \n",
            "                                                                                                                        \n",
            " Encoder-11-MultiHeadSelfAttention-Dro  (None, None, 768)         0             ['Encoder-11-MultiHeadSelfAttention[0][0\n",
            " pout (Dropout)                                                                 ]']                                     \n",
            "                                                                                                                        \n",
            " Encoder-11-MultiHeadSelfAttention-Add  (None, None, 768)         0             ['Encoder-10-FeedForward-Norm[0][0]',   \n",
            "  (Add)                                                                          'Encoder-11-MultiHeadSelfAttention-Drop\n",
            "                                                                                out[0][0]']                             \n",
            "                                                                                                                        \n",
            " Encoder-11-MultiHeadSelfAttention-Nor  (None, None, 768)         1536          ['Encoder-11-MultiHeadSelfAttention-Add[\n",
            " m (LayerNormalization)                                                         0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-11-FeedForward (FeedForward)  (None, None, 768)          4722432       ['Encoder-11-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]']                                \n",
            "                                                                                                                        \n",
            " Encoder-11-FeedForward-Dropout (Dropo  (None, None, 768)         0             ['Encoder-11-FeedForward[0][0]']        \n",
            " ut)                                                                                                                    \n",
            "                                                                                                                        \n",
            " Encoder-11-FeedForward-Add (Add)      (None, None, 768)          0             ['Encoder-11-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]',                                \n",
            "                                                                                 'Encoder-11-FeedForward-Dropout[0][0]']\n",
            "                                                                                                                        \n",
            " Encoder-11-FeedForward-Norm (LayerNor  (None, None, 768)         1536          ['Encoder-11-FeedForward-Add[0][0]']    \n",
            " malization)                                                                                                            \n",
            "                                                                                                                        \n",
            " Encoder-12-MultiHeadSelfAttention (Mu  (None, None, 768)         2362368       ['Encoder-11-FeedForward-Norm[0][0]']   \n",
            " ltiHeadAttention)                                                                                                      \n",
            "                                                                                                                        \n",
            " Encoder-12-MultiHeadSelfAttention-Dro  (None, None, 768)         0             ['Encoder-12-MultiHeadSelfAttention[0][0\n",
            " pout (Dropout)                                                                 ]']                                     \n",
            "                                                                                                                        \n",
            " Encoder-12-MultiHeadSelfAttention-Add  (None, None, 768)         0             ['Encoder-11-FeedForward-Norm[0][0]',   \n",
            "  (Add)                                                                          'Encoder-12-MultiHeadSelfAttention-Drop\n",
            "                                                                                out[0][0]']                             \n",
            "                                                                                                                        \n",
            " Encoder-12-MultiHeadSelfAttention-Nor  (None, None, 768)         1536          ['Encoder-12-MultiHeadSelfAttention-Add[\n",
            " m (LayerNormalization)                                                         0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-12-FeedForward (FeedForward)  (None, None, 768)          4722432       ['Encoder-12-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]']                                \n",
            "                                                                                                                        \n",
            " Encoder-12-FeedForward-Dropout (Dropo  (None, None, 768)         0             ['Encoder-12-FeedForward[0][0]']        \n",
            " ut)                                                                                                                    \n",
            "                                                                                                                        \n",
            " Encoder-12-FeedForward-Add (Add)      (None, None, 768)          0             ['Encoder-12-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]',                                \n",
            "                                                                                 'Encoder-12-FeedForward-Dropout[0][0]']\n",
            "                                                                                                                        \n",
            " Encoder-12-FeedForward-Norm (LayerNor  (None, None, 768)         1536          ['Encoder-12-FeedForward-Add[0][0]']    \n",
            " malization)                                                                                                            \n",
            "                                                                                                                        \n",
            "========================================================================================================================\n",
            "Total params: 101,677,056\n",
            "Trainable params: 0\n",
            "Non-trainable params: 101,677,056\n",
            "________________________________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_path = get_pretrained(PretrainedList.chinese_base)\n",
        "paths = get_checkpoint_paths(model_path)\n",
        "bert_model = load_trained_model_from_checkpoint(\n",
        "    paths.config, paths.checkpoint, training=False, seq_len=None\n",
        ")\n",
        "bert_model.summary(line_length=120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EMQNIppDX6lt"
      },
      "outputs": [],
      "source": [
        "token_dict = load_vocabulary(paths.vocab)\n",
        "len(token_dict)\n",
        "tokenizer = Tokenizer(token_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw9v_j6FdNBP"
      },
      "source": [
        "載入打散後資料"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI3L2ujxdHmY"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('x_train.pickle', 'rb') as f:\n",
        "    x_text = pickle.load(f)\n",
        "with open('y_train.pickle', 'rb') as f:\n",
        "    y_tag = pickle.load(f)\n",
        "\n",
        "X = x_text[:]\n",
        "Y = y_tag[:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "a5a2KyoNfR9o"
      },
      "outputs": [],
      "source": [
        "#設置模型\n",
        "def build_model():\n",
        "    #bert_model.summary(line_length=120)\n",
        "    model_path = get_pretrained(PretrainedList.chinese_base)\n",
        "    paths = get_checkpoint_paths(model_path)\n",
        "    bert_model = load_trained_model_from_checkpoint(\n",
        "     paths.config, paths.checkpoint, training=False, seq_len=None\n",
        "    )\n",
        "    #bert Tokenizer\n",
        "    token_dict = load_vocabulary(paths.vocab)\n",
        "    len(token_dict)\n",
        "    tokenizer = Tokenizer(token_dict)\n",
        "    # 把 BERT 模型裡的每一層的trainable屬性都設成True，讓BERT模型裡的權重都可以被訓練\n",
        "    for l in bert_model.layers:\n",
        "        l.trainable = True\n",
        "\n",
        "    # 模型的輸入，我們會需要兩個輸入，分別是語句內容以及其對應的segments\n",
        "    x1_in = Input(shape=(None,))\n",
        "    x2_in = Input(shape=(None,))\n",
        "    x = bert_model([x1_in, x2_in])\n",
        "    x = Lambda(lambda x: x[:, 0])(x) # 取出[CLS]對應位置的向量出來\n",
        "    p = Dense(18, activation='softmax')(x) # 輸出情緒傾向數值\n",
        "\n",
        "    model = Model([x1_in, x2_in], p)\n",
        "\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=Adam(1e-5),    #用較小的學習率\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "id": "AtZ4KPqMaWya",
        "outputId": "7578259d-dd36-406f-a100-f46130a0f0d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing fold # 0\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " model_3 (Functional)           (None, None, 768)    101677056   ['input_1[0][0]',                \n",
            "                                                                  'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 768)          0           ['model_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 18)           13842       ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 101,690,898\n",
            "Trainable params: 101,690,898\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d26ab3ac420d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     history = model.fit(\n\u001b[1;32m     38\u001b[0m         \u001b[0mpartial_train_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpartial_train_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         batch_size=BATCH_SIZE,epochs=EPOCHS,verbose=0)\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;31m# evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "k=10\n",
        "num_val_samples = len(X) // k\n",
        "cvscores = []\n",
        "results = []\n",
        "for i in range(k):\n",
        "    print('processing fold #',i)\n",
        "    test_data = X[i*num_val_samples:(i+1)*num_val_samples]\n",
        "    tag_test = Y[i*num_val_samples:(i+1)*num_val_samples]\n",
        "    train_data = np.concatenate(\n",
        "        [X[:i*num_val_samples],X[(i+1)*num_val_samples:]],axis=0)\n",
        "    tag_train = np.concatenate(\n",
        "        [Y[:i*num_val_samples],Y[(i+1)*num_val_samples:]],axis=0)\n",
        "\n",
        "    partial_train_data = []\n",
        "    partial_train_targets = []\n",
        "    val_data = []\n",
        "    val_targets = []\n",
        "\n",
        "    indices = []\n",
        "    for sentence in train_data:\n",
        "        ids, segments = tokenizer.encode(sentence, max_len=512) # Tokenizer進行分詞\n",
        "        indices.append(ids)\n",
        "    indices = np.array(indices)\n",
        "    partial_train_data = [indices, np.zeros_like(indices)]\n",
        "    partial_train_targets = np.array(tag_train)\n",
        "\n",
        "    indices = []\n",
        "    for sentence in test_data:\n",
        "        ids, segments = tokenizer.encode(sentence, max_len=512) # Tokenizer進行分詞\n",
        "        indices.append(ids)\n",
        "    indices = np.array(indices)\n",
        "    val_data = [indices, np.zeros_like(indices)]\n",
        "    val_targets = np.array(tag_test)\n",
        "    \n",
        "    model = build_model()   \n",
        "    \n",
        "    history = model.fit(\n",
        "        partial_train_data,partial_train_targets,\n",
        "        batch_size=BATCH_SIZE,epochs=EPOCHS,verbose=0)\n",
        "# evaluate the model\n",
        "    scores = model.evaluate(val_data,val_targets,batch_size=BATCH_SIZE, verbose=1) \n",
        "    result = model.predict(val_data)\n",
        "    results.append(result)\n",
        "    cvscores.append(scores[1] * 100)\n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
        "print(\"End of Predict DataSet!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjNAlnvxagDV"
      },
      "outputs": [],
      "source": [
        "model.save('bert200-k10_6310.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RecxoCJbarla"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('bert200_k10_results.pickle', 'wb') as f:\n",
        "    pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVk79gcxhLeQ"
      },
      "outputs": [],
      "source": [
        "# convert the history.history dict to a pandas DataFrame:  \n",
        "import pandas as pd   \n",
        "hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "# save to json:  \n",
        "hist_json_file = 'history200_k10.json' \n",
        "with open(hist_json_file, mode='w') as f:\n",
        "    hist_df.to_json(f)\n",
        "\n",
        "# or save to csv: \n",
        "hist_csv_file = 'history200_k10.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 載入模型"
      ],
      "metadata": {
        "id": "TnZUJ7gppZcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model()  \n",
        "model.load_weights('bert200-k10_6310.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc4PbGcxpHy3",
        "outputId": "128935d3-dd54-4016-aa34-f7469a303f30"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " model_3 (Functional)           (None, None, 768)    101677056   ['input_1[0][0]',                \n",
            "                                                                  'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 768)          0           ['model_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 18)           13842       ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 101,690,898\n",
            "Trainable params: 101,690,898\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 實際上來用用看，看看它對於輸出的預測是什麼：\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QveIKuNjphss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_text = '大智慧的人，他的心非常的安詳，他過著世界太平的日子。應該無所著，生起清淨心。為什麼？世間，不值得我們執著。不值得我們這樣痛苦，煎熬自己。這是很愚痴的人，但問題是習氣難改。'\n",
        "\n",
        "ids, segments = tokenizer.encode(sentiment_text, max_len=512)\n",
        "\n",
        "my_X_test = [np.array([ids]), np.array([segments])]\n",
        "\n",
        "predict = model.predict(my_X_test)\n",
        "LabelRank = np.argsort(predict)\n",
        "print(predict)\n",
        "print(LabelRank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqrD-TsPpgAc",
        "outputId": "a022791c-899f-4c4f-cd6b-d3ad66c4638d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9.6177295e-02 2.8489102e-02 1.0365417e-03 7.9962951e-01 2.5823817e-03\n",
            "  5.6395256e-05 2.5041523e-05 2.1959697e-03 6.8125874e-03 1.4742593e-03\n",
            "  2.8604770e-04 5.4539654e-02 3.7794972e-03 9.1733108e-04 9.8394428e-04\n",
            "  6.0082297e-04 1.1074905e-04 3.0287652e-04]]\n",
            "[[ 6  5 16 10 17 15 13 14  2  9  7  4 12  8  1 11  0  3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Buddha_Dic = {'0':'一切有為法，如夢幻泡影','1':'凡所有相，皆是虛妄','2':'若真修道人，不見世間過','3':'應無所住，而生其心',\n",
        "              '4':'過去心不可得，現在心不可得','5':'色不異空，空不異色','6':'不生不滅，不垢不淨','7':'不取於相，如如不動',\n",
        "              '8':'如來者，無所從來','9':'無有定法，如來可說','10':'無有恐怖，遠離顛倒夢想','11':'法尚應捨，何況非法',\n",
        "              '12':'本來無一物，何處惹塵埃','13':'一念悟時，眾生是佛','14':'何期自性，本自清淨','15':'不是幡動，仁者心動',\n",
        "              '16':'真語者，實語者','17':'是法平等，無有高下'}  "
      ],
      "metadata": {
        "id": "8lZiHemsyydD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "MyBert6360_k10_200.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}