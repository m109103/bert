{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "「MyBert.ipynb」的副本",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOq9aoBH1JlG9dMyTGicf9m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m109103/bert/blob/main/MyBert_B5E10Line120Max300.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-bert"
      ],
      "metadata": {
        "id": "Z1RbKNCjqUV1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "640fc8fc-6336-461f-d580-a1269f87a5c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-bert\n",
            "  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-bert) (1.21.6)\n",
            "Collecting keras-transformer==0.40.0\n",
            "  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
            "Collecting keras-pos-embd==0.13.0\n",
            "  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
            "Collecting keras-multi-head==0.29.0\n",
            "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
            "Collecting keras-layer-normalization==0.16.0\n",
            "  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
            "Collecting keras-position-wise-feed-forward==0.8.0\n",
            "  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
            "Collecting keras-embed-sim==0.10.0\n",
            "  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
            "Collecting keras-self-attention==0.51.0\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33517 sha256=a2db15ef45c1ad19bfff75573185c58b647be98c8d36c9d9da54272a42060070\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/e8/45/842b3a39831261aef9154b907eacbc4ac99499a99ae829b06f\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12305 sha256=823d05a42379c62af93ba29b3f0df665a3c18989ca3d257fefe3fadda5f53c0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/68/26/692ed21edd832833c3b0a0e21615bcacd99ca458b3f9ed571f\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3960 sha256=d9a3683dc80d44f4d8ee0a13fcc36daa848aa93031222b10b1dca2e6c3c609e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/67/b5/d847588d075895281e1cf5590f819bd4cf076a554872268bd5\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4668 sha256=0cf2b936347a014200af3aa0d7ea2c518034057d86ecd0b5f00ac70c85d4962f\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/5d/1c/2e619f594f69fbcf8bc20943b27d414871c409be053994813e\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14993 sha256=a6200908ecbbc876438e4c83485916005368081ce9ac9951f07a40a979dc6e76\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/aa/3c/9d15d24005179dae08ff291ce99c754b296347817d076fd9fb\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6962 sha256=a7c3f86b2bd073ab651ae04f134c86a73407adf78949f44ea8ab2ca7555b6c96\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/c1/a0/dc44fcf68c857b7ff6be9a97e675e5adf51022eff1169b042f\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4983 sha256=294affca626cd6ea0192a09fa01988c9516f96f0496fd0ed4134e30604112a0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/75/6f/d42f6e051506f442daeba53ff1e2d21a5f20ef8c411610f2bb\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=b6c1abcd40f13d0a274fe541031b9706f8374ffa35370061dd277b8599b006c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
            "Successfully built keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention\n",
            "Installing collected packages: keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.89.0 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "iCTpza-0LRzB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4cae7d0b-a51c-4b19-af80-a60414ef527d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "metadata": {
        "id": "3y0Vg7WwLkZN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "22cff16f-4128-4f2f-fde6-e366b0d5d01a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "5.570024063000005\n",
            "GPU (s):\n",
            "0.11532551099998045\n",
            "GPU speedup over CPU: 48x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall keras-nightly"
      ],
      "metadata": {
        "id": "KMHVNi43y1wo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall -y tensorflow"
      ],
      "metadata": {
        "id": "xG5Bulg4zAas"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tensorflow-gpu==2.2.0"
      ],
      "metadata": {
        "id": "mg4doAqjzNsx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install keras==2.3.0"
      ],
      "metadata": {
        "id": "2gyR3gzezglz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install keras-bert==0.82.0"
      ],
      "metadata": {
        "id": "OnVKby32zvXg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!nvidia-smi"
      ],
      "metadata": {
        "id": "nWoWoFQhz335"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive  \n",
        "drive.mount('/content/gdrive')  "
      ],
      "metadata": {
        "id": "mQChBWn7C2oP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "09632019-7cc4-4949-9dba-df0dbaff93d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from keras_bert import (\n",
        "    load_vocabulary,\n",
        "    load_trained_model_from_checkpoint,\n",
        "    Tokenizer,\n",
        "    get_checkpoint_paths,\n",
        ")\n",
        "from keras_bert.datasets import get_pretrained, PretrainedList"
      ],
      "metadata": {
        "id": "PZOcUU4i0IsC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SPLIT_PORTION = 0.8  # 指定訓練資料與測試資料的比例\n",
        "BATCH_SIZE = 5  # batch size建議不要設得太大，不然很有可能out of memory\n",
        "EPOCHS = 10 # epoch 5次其實就很夠了，當然你可以嘗試再大一點，只是訓練要更久"
      ],
      "metadata": {
        "id": "ujxw-JrC0OHS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = get_pretrained(PretrainedList.chinese_base)\n",
        "paths = get_checkpoint_paths(model_path)\n",
        "bert_model = load_trained_model_from_checkpoint(\n",
        "    paths.config, paths.checkpoint, training=False, seq_len=None\n",
        ")\n",
        "bert_model.summary(line_length=120)"
      ],
      "metadata": {
        "id": "RW01f8ZM0ZyT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "cd723107-6f2e-4172-ce11-2c1ae02119fc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n",
            "381894656/381892918 [==============================] - 5s 0us/step\n",
            "381902848/381892918 [==============================] - 5s 0us/step\n",
            "Model: \"model_1\"\n",
            "________________________________________________________________________________________________________________________\n",
            " Layer (type)                          Output Shape               Param #       Connected to                            \n",
            "========================================================================================================================\n",
            " Input-Token (InputLayer)              [(None, None)]             0             []                                      \n",
            "                                                                                                                        \n",
            " Input-Segment (InputLayer)            [(None, None)]             0             []                                      \n",
            "                                                                                                                        \n",
            " Embedding-Token (TokenEmbedding)      [(None, None, 768),        16226304      ['Input-Token[0][0]']                   \n",
            "                                        (21128, 768)]                                                                   \n",
            "                                                                                                                        \n",
            " Embedding-Segment (Embedding)         (None, None, 768)          1536          ['Input-Segment[0][0]']                 \n",
            "                                                                                                                        \n",
            " Embedding-Token-Segment (Add)         (None, None, 768)          0             ['Embedding-Token[0][0]',               \n",
            "                                                                                 'Embedding-Segment[0][0]']             \n",
            "                                                                                                                        \n",
            " Embedding-Position (PositionEmbedding  (None, None, 768)         393216        ['Embedding-Token-Segment[0][0]']       \n",
            " )                                                                                                                      \n",
            "                                                                                                                        \n",
            " Embedding-Dropout (Dropout)           (None, None, 768)          0             ['Embedding-Position[0][0]']            \n",
            "                                                                                                                        \n",
            " Embedding-Norm (LayerNormalization)   (None, None, 768)          1536          ['Embedding-Dropout[0][0]']             \n",
            "                                                                                                                        \n",
            " Encoder-1-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Embedding-Norm[0][0]']                \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-1-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-1-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-1-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Embedding-Norm[0][0]',                \n",
            " (Add)                                                                           'Encoder-1-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-1-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-1-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-1-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-1-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-1-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-1-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-1-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-1-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-1-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-1-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-1-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-2-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-1-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-2-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-2-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-2-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-1-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-2-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-2-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-2-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-2-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-2-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-2-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-2-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-2-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-2-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-2-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-2-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-2-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-3-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-2-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-3-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-3-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-3-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-2-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-3-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-3-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-3-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-3-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-3-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-3-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-3-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-3-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-3-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-3-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-3-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-3-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-4-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-3-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-4-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-4-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-4-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-3-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-4-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-4-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-4-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-4-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-4-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-4-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-4-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-4-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-4-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-4-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-4-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-4-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-5-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-4-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-5-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-5-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-5-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-4-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-5-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-5-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-5-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-5-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-5-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-5-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-5-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-5-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-5-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-5-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-5-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-5-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-6-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-5-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-6-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-6-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-6-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-5-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-6-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-6-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-6-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-6-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-6-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-6-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-6-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-6-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-6-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-6-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-6-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-6-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-7-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-6-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-7-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-7-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-7-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-6-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-7-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-7-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-7-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-7-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-7-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-7-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-7-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-7-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-7-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-7-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-7-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-7-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-8-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-7-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-8-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-8-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-8-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-7-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-8-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-8-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-8-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-8-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-8-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-8-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-8-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-8-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-8-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-8-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-8-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-8-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-9-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-8-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-9-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-9-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-9-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-8-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-9-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-9-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-9-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-9-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-9-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-9-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-9-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-9-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-9-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-9-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-9-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-9-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-10-MultiHeadSelfAttention (Mu  (None, None, 768)         2362368       ['Encoder-9-FeedForward-Norm[0][0]']    \n",
            " ltiHeadAttention)                                                                                                      \n",
            "                                                                                                                        \n",
            " Encoder-10-MultiHeadSelfAttention-Dro  (None, None, 768)         0             ['Encoder-10-MultiHeadSelfAttention[0][0\n",
            " pout (Dropout)                                                                 ]']                                     \n",
            "                                                                                                                        \n",
            " Encoder-10-MultiHeadSelfAttention-Add  (None, None, 768)         0             ['Encoder-9-FeedForward-Norm[0][0]',    \n",
            "  (Add)                                                                          'Encoder-10-MultiHeadSelfAttention-Drop\n",
            "                                                                                out[0][0]']                             \n",
            "                                                                                                                        \n",
            " Encoder-10-MultiHeadSelfAttention-Nor  (None, None, 768)         1536          ['Encoder-10-MultiHeadSelfAttention-Add[\n",
            " m (LayerNormalization)                                                         0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-10-FeedForward (FeedForward)  (None, None, 768)          4722432       ['Encoder-10-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]']                                \n",
            "                                                                                                                        \n",
            " Encoder-10-FeedForward-Dropout (Dropo  (None, None, 768)         0             ['Encoder-10-FeedForward[0][0]']        \n",
            " ut)                                                                                                                    \n",
            "                                                                                                                        \n",
            " Encoder-10-FeedForward-Add (Add)      (None, None, 768)          0             ['Encoder-10-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]',                                \n",
            "                                                                                 'Encoder-10-FeedForward-Dropout[0][0]']\n",
            "                                                                                                                        \n",
            " Encoder-10-FeedForward-Norm (LayerNor  (None, None, 768)         1536          ['Encoder-10-FeedForward-Add[0][0]']    \n",
            " malization)                                                                                                            \n",
            "                                                                                                                        \n",
            " Encoder-11-MultiHeadSelfAttention (Mu  (None, None, 768)         2362368       ['Encoder-10-FeedForward-Norm[0][0]']   \n",
            " ltiHeadAttention)                                                                                                      \n",
            "                                                                                                                        \n",
            " Encoder-11-MultiHeadSelfAttention-Dro  (None, None, 768)         0             ['Encoder-11-MultiHeadSelfAttention[0][0\n",
            " pout (Dropout)                                                                 ]']                                     \n",
            "                                                                                                                        \n",
            " Encoder-11-MultiHeadSelfAttention-Add  (None, None, 768)         0             ['Encoder-10-FeedForward-Norm[0][0]',   \n",
            "  (Add)                                                                          'Encoder-11-MultiHeadSelfAttention-Drop\n",
            "                                                                                out[0][0]']                             \n",
            "                                                                                                                        \n",
            " Encoder-11-MultiHeadSelfAttention-Nor  (None, None, 768)         1536          ['Encoder-11-MultiHeadSelfAttention-Add[\n",
            " m (LayerNormalization)                                                         0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-11-FeedForward (FeedForward)  (None, None, 768)          4722432       ['Encoder-11-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]']                                \n",
            "                                                                                                                        \n",
            " Encoder-11-FeedForward-Dropout (Dropo  (None, None, 768)         0             ['Encoder-11-FeedForward[0][0]']        \n",
            " ut)                                                                                                                    \n",
            "                                                                                                                        \n",
            " Encoder-11-FeedForward-Add (Add)      (None, None, 768)          0             ['Encoder-11-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]',                                \n",
            "                                                                                 'Encoder-11-FeedForward-Dropout[0][0]']\n",
            "                                                                                                                        \n",
            " Encoder-11-FeedForward-Norm (LayerNor  (None, None, 768)         1536          ['Encoder-11-FeedForward-Add[0][0]']    \n",
            " malization)                                                                                                            \n",
            "                                                                                                                        \n",
            " Encoder-12-MultiHeadSelfAttention (Mu  (None, None, 768)         2362368       ['Encoder-11-FeedForward-Norm[0][0]']   \n",
            " ltiHeadAttention)                                                                                                      \n",
            "                                                                                                                        \n",
            " Encoder-12-MultiHeadSelfAttention-Dro  (None, None, 768)         0             ['Encoder-12-MultiHeadSelfAttention[0][0\n",
            " pout (Dropout)                                                                 ]']                                     \n",
            "                                                                                                                        \n",
            " Encoder-12-MultiHeadSelfAttention-Add  (None, None, 768)         0             ['Encoder-11-FeedForward-Norm[0][0]',   \n",
            "  (Add)                                                                          'Encoder-12-MultiHeadSelfAttention-Drop\n",
            "                                                                                out[0][0]']                             \n",
            "                                                                                                                        \n",
            " Encoder-12-MultiHeadSelfAttention-Nor  (None, None, 768)         1536          ['Encoder-12-MultiHeadSelfAttention-Add[\n",
            " m (LayerNormalization)                                                         0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-12-FeedForward (FeedForward)  (None, None, 768)          4722432       ['Encoder-12-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]']                                \n",
            "                                                                                                                        \n",
            " Encoder-12-FeedForward-Dropout (Dropo  (None, None, 768)         0             ['Encoder-12-FeedForward[0][0]']        \n",
            " ut)                                                                                                                    \n",
            "                                                                                                                        \n",
            " Encoder-12-FeedForward-Add (Add)      (None, None, 768)          0             ['Encoder-12-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]',                                \n",
            "                                                                                 'Encoder-12-FeedForward-Dropout[0][0]']\n",
            "                                                                                                                        \n",
            " Encoder-12-FeedForward-Norm (LayerNor  (None, None, 768)         1536          ['Encoder-12-FeedForward-Add[0][0]']    \n",
            " malization)                                                                                                            \n",
            "                                                                                                                        \n",
            "========================================================================================================================\n",
            "Total params: 101,677,056\n",
            "Trainable params: 0\n",
            "Non-trainable params: 101,677,056\n",
            "________________________________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_dict = load_vocabulary(paths.vocab)\n",
        "len(token_dict)\n",
        "tokenizer = Tokenizer(token_dict)"
      ],
      "metadata": {
        "id": "uQY0yd0PPsVy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/\"My Drive\"/\"Colab Notebooks\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EMQNIppDX6lt",
        "outputId": "10b0c30c-724a-41db-eabe-c6d07ace1ca5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('./data/bert_fs.json', 'r', encoding='utf-8') as f1:\n",
        "    fs = json.load(f1)\n",
        "with open('./data/bert_dic.json', 'r', encoding='utf-8') as f2:\n",
        "    new_dic = json.load(f2)\n",
        "with open('./data/bert_texts.json', 'r', encoding='utf-8') as f3:\n",
        "    text = json.load(f3)\n",
        "with open('./data/bert_tags.json', 'r', encoding='utf-8') as f4:\n",
        "    tag = json.load(f4)"
      ],
      "metadata": {
        "id": "a2zR66hcV4iE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = text[:]\n",
        "y = tag[:]\n",
        "# 把文章資料相對標籤資料的順序打亂\n",
        "def call_random(text_list,label_list):\n",
        "    import numpy as np\n",
        "    state = np.random.get_state()\n",
        "    np.random.shuffle(text_list)\n",
        "    np.random.set_state(state)\n",
        "    np.random.shuffle(label_list)\n",
        "    return text_list,label_list\n",
        "#random 後的資料\n",
        "x_text,y_tag = call_random(x,y)"
      ],
      "metadata": {
        "id": "F9cvfL4xZCgJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle    \n",
        "with open('./data/r_text.pickle', 'wb') as f:\n",
        "    pickle.dump(x_text, f)\n",
        "with open('./data/r_tag.pickle', 'wb') as f:\n",
        "    pickle.dump(y_tag, f)"
      ],
      "metadata": {
        "id": "bdQ4PtX6ZQXu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "載入打散後資料"
      ],
      "metadata": {
        "id": "Aw9v_j6FdNBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import pickle\n",
        "#with open('./data/r_text.pickle', 'rb') as f:\n",
        "#    x_text = pickle.load(f)\n",
        "#with open('./data/r_tag.pickle', 'rb') as f:\n",
        " #   y_tag = pickle.load(f)"
      ],
      "metadata": {
        "id": "lI3L2ujxdHmY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = x_text[:]\n",
        "tag_data = y_tag[:]\n",
        "\n",
        "# 按比例切分訓練資料與測試資料\n",
        "train_data = text_data[:int(len(text_data) * SPLIT_PORTION)]\n",
        "test_data = text_data[int(len(text_data) * SPLIT_PORTION):] \n",
        "tag_train = tag_data[:int(len(tag_data) * SPLIT_PORTION)]\n",
        "tag_test = tag_data[int(len(tag_data) * SPLIT_PORTION):]\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "indices = []\n",
        "for sentence in train_data:\n",
        "    ids, segments = tokenizer.encode(sentence, max_len=300) # Tokenizer進行分詞\n",
        "    indices.append(ids)\n",
        "indices = np.array(indices)\n",
        "X_train = [indices, np.zeros_like(indices)]\n",
        "y_train = np.array(tag_train)\n",
        "\n",
        "indices = []\n",
        "for sentence in test_data:\n",
        "    ids, segments = tokenizer.encode(sentence, max_len=300) # Tokenizer進行分詞\n",
        "    indices.append(ids)\n",
        "indices = np.array(indices)\n",
        "X_test = [indices, np.zeros_like(indices)]\n",
        "y_test = np.array(tag_test)"
      ],
      "metadata": {
        "id": "6FKU3vBaZlrC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 把 BERT 模型裡的每一層的trainable屬性都設成True，讓BERT模型裡的權重都可以被訓練\n",
        "for l in bert_model.layers:\n",
        "    l.trainable = True\n",
        "\n",
        "# 模型的輸入，我們會需要兩個輸入，分別是語句內容以及其對應的segments\n",
        "x1_in = Input(shape=(None,))\n",
        "x2_in = Input(shape=(None,))\n",
        "\n",
        "x = bert_model([x1_in, x2_in])\n",
        "x = Lambda(lambda x: x[:, 0])(x) # 取出[CLS]對應位置的向量出來\n",
        "p = Dense(18, activation='softmax')(x) # 輸出情緒傾向數值\n",
        "\n",
        "model = Model([x1_in, x2_in], p)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=Adam(1e-5),    #用較小的學習率\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "9pPbKA0HaEEh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8a42bc89-f3cc-4b62-9952-ca0056e17874"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " model_1 (Functional)           (None, None, 768)    101677056   ['input_1[0][0]',                \n",
            "                                                                  'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 768)          0           ['model_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 18)           13842       ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 101,690,898\n",
            "Trainable params: 101,690,898\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.2\n",
        ")"
      ],
      "metadata": {
        "id": "AtZ4KPqMaWya",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "66c64e6d-5ef2-4e50-d1ac-ea99a89cf775"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "809/809 [==============================] - 362s 423ms/step - loss: 1.0976 - accuracy: 0.6870 - val_loss: 0.5389 - val_accuracy: 0.8576\n",
            "Epoch 2/10\n",
            "809/809 [==============================] - 340s 421ms/step - loss: 0.3389 - accuracy: 0.9043 - val_loss: 0.3887 - val_accuracy: 0.8892\n",
            "Epoch 3/10\n",
            "809/809 [==============================] - 340s 420ms/step - loss: 0.1270 - accuracy: 0.9688 - val_loss: 0.4076 - val_accuracy: 0.8942\n",
            "Epoch 4/10\n",
            "809/809 [==============================] - 340s 420ms/step - loss: 0.0682 - accuracy: 0.9809 - val_loss: 0.4771 - val_accuracy: 0.8971\n",
            "Epoch 5/10\n",
            "809/809 [==============================] - 340s 420ms/step - loss: 0.0319 - accuracy: 0.9936 - val_loss: 0.4491 - val_accuracy: 0.8961\n",
            "Epoch 6/10\n",
            "809/809 [==============================] - 340s 420ms/step - loss: 0.0411 - accuracy: 0.9889 - val_loss: 0.5251 - val_accuracy: 0.8813\n",
            "Epoch 7/10\n",
            "809/809 [==============================] - 340s 420ms/step - loss: 0.0301 - accuracy: 0.9916 - val_loss: 0.5163 - val_accuracy: 0.8803\n",
            "Epoch 8/10\n",
            "809/809 [==============================] - 339s 420ms/step - loss: 0.0162 - accuracy: 0.9958 - val_loss: 0.4858 - val_accuracy: 0.8932\n",
            "Epoch 9/10\n",
            "809/809 [==============================] - 340s 420ms/step - loss: 0.0293 - accuracy: 0.9911 - val_loss: 0.5398 - val_accuracy: 0.8971\n",
            "Epoch 10/10\n",
            "809/809 [==============================] - 340s 420ms/step - loss: 0.0086 - accuracy: 0.9990 - val_loss: 0.5311 - val_accuracy: 0.9001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('bert120-m300.h5')"
      ],
      "metadata": {
        "id": "zjNAlnvxagDV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(X_test, y_test) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "t78mw62yanVc",
        "outputId": "2bdf121c-0fae-4499-d732-43520d1334a7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 30s 758ms/step - loss: 0.5580 - accuracy: 0.9019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(scores[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RecxoCJbarla",
        "outputId": "034a7e96-47cc-487f-d68f-8f2aaf7df314"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9018987417221069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('results_bert120m300.pickle','wb') as f:\n",
        "  pickle.dump(scores,f)"
      ],
      "metadata": {
        "id": "f6YNjCFjTI_s"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import pickle\n",
        "#with open('results_bert.pickle','rb') as f:\n",
        "#  scores = pickle.load(f)"
      ],
      "metadata": {
        "id": "PQdXLABmTNN8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = model.predict(X_test)"
      ],
      "metadata": {
        "id": "zCBSu5zExxPB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(p[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "r21FA0EQIytm",
        "outputId": "c142558b-a944-4e43-a966-82dee0219f7f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8.0791909e-07 3.7588360e-04 7.6136248e-06 9.9795830e-01 6.4969106e-05\n",
            " 5.1334368e-06 3.8012038e-07 1.4966731e-06 2.2170102e-06 1.5828459e-06\n",
            " 2.4895930e-05 9.7634783e-04 5.5041286e-04 7.8121639e-06 4.2858858e-07\n",
            " 4.6448085e-06 2.1503497e-06 1.4987778e-05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('p_b5e10bert120.pickle','wb') as f:\n",
        "  pickle.dump(p,f)\n",
        "with open('l_x_test.pickle','wb') as lf:\n",
        "  pickle.dump(y_test,lf)"
      ],
      "metadata": {
        "id": "ggaNVio66g4e"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}