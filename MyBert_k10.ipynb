{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m109103/bert/blob/main/MyBert_k10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g1lnuesyxCqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "378a4eca-eecb-4f1e-f6be-c8aa72de04cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-bert\n",
            "  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-bert) (1.21.6)\n",
            "Collecting keras-transformer==0.40.0\n",
            "  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
            "Collecting keras-pos-embd==0.13.0\n",
            "  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
            "Collecting keras-multi-head==0.29.0\n",
            "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
            "Collecting keras-layer-normalization==0.16.0\n",
            "  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
            "Collecting keras-position-wise-feed-forward==0.8.0\n",
            "  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
            "Collecting keras-embed-sim==0.10.0\n",
            "  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
            "Collecting keras-self-attention==0.51.0\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33517 sha256=6b7ac18c65c812faf5ca1534dae379f9e7c6e5fbe94e2ff4ce1986ad6b96a8e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/e8/45/842b3a39831261aef9154b907eacbc4ac99499a99ae829b06f\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12305 sha256=582f0f1fc4b4d7cbb28908e98f8955af46dc2d7b5fd2c1d05bda2d9102bbd1ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/68/26/692ed21edd832833c3b0a0e21615bcacd99ca458b3f9ed571f\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3960 sha256=4b009ec7bdd52127ab5ac8e94972fc07cf346c9f062ecf621b8e45cafc12261f\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/67/b5/d847588d075895281e1cf5590f819bd4cf076a554872268bd5\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4668 sha256=916edc66f8eb733fb5c48936cde346935cc292c0b73fbcca99285e8414a73866\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/5d/1c/2e619f594f69fbcf8bc20943b27d414871c409be053994813e\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14993 sha256=b87b292bb7f843175fd5ae9259a312956643edd61576e5e06770c3849106909f\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/aa/3c/9d15d24005179dae08ff291ce99c754b296347817d076fd9fb\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6962 sha256=82789b14b6cb7b784e1119523e14a8f5bf488bb2dbd42abf281087e2847ce8bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/c1/a0/dc44fcf68c857b7ff6be9a97e675e5adf51022eff1169b042f\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4983 sha256=7b176c6d61be1536b9767385737da8199740f0d948bf26eeac9a7e6cdf7be866\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/75/6f/d42f6e051506f442daeba53ff1e2d21a5f20ef8c411610f2bb\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=7c0b8bffc5a96c5011d0f0ad854c41edc28ce45cce2fa488d769cd8a53a41cc1\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
            "Successfully built keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention\n",
            "Installing collected packages: keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.89.0 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iCTpza-0LRzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f15220e-7714-4c00-83bf-75ac9d0e19e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May 31 02:37:21 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y0Vg7WwLkZN",
        "outputId": "527dc79c-306f-4325-b696-f433a2864799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "1.4297627200000136\n",
            "GPU (s):\n",
            "0.036829251999989765\n",
            "GPU speedup over CPU: 38x\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mQChBWn7C2oP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd49a0e1-f187-4afc-fce6-25b2101da1ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive  \n",
        "drive.mount('/content/gdrive')  \n",
        "%cd /content/gdrive/\"My Drive\"/\"Colab Notebooks\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PZOcUU4i0IsC"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from keras_bert import (\n",
        "    load_vocabulary,\n",
        "    load_trained_model_from_checkpoint,\n",
        "    Tokenizer,\n",
        "    get_checkpoint_paths,\n",
        ")\n",
        "from keras_bert.datasets import get_pretrained, PretrainedList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ujxw-JrC0OHS"
      },
      "outputs": [],
      "source": [
        "# 指定訓練資料與測試資料的比例\n",
        "BATCH_SIZE = 10 # batch size建議不要設得太大，不然很有可能out of memory\n",
        "EPOCHS = 5 # epoch 5次其實就很夠了，當然你可以嘗試再大一點，只是訓練要更久"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RW01f8ZM0ZyT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "755ba1c9-3d14-4c9c-e3ec-46af155d74f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n",
            "381894656/381892918 [==============================] - 5s 0us/step\n",
            "381902848/381892918 [==============================] - 5s 0us/step\n",
            "Model: \"model_1\"\n",
            "________________________________________________________________________________________________________________________\n",
            " Layer (type)                          Output Shape               Param #       Connected to                            \n",
            "========================================================================================================================\n",
            " Input-Token (InputLayer)              [(None, None)]             0             []                                      \n",
            "                                                                                                                        \n",
            " Input-Segment (InputLayer)            [(None, None)]             0             []                                      \n",
            "                                                                                                                        \n",
            " Embedding-Token (TokenEmbedding)      [(None, None, 768),        16226304      ['Input-Token[0][0]']                   \n",
            "                                        (21128, 768)]                                                                   \n",
            "                                                                                                                        \n",
            " Embedding-Segment (Embedding)         (None, None, 768)          1536          ['Input-Segment[0][0]']                 \n",
            "                                                                                                                        \n",
            " Embedding-Token-Segment (Add)         (None, None, 768)          0             ['Embedding-Token[0][0]',               \n",
            "                                                                                 'Embedding-Segment[0][0]']             \n",
            "                                                                                                                        \n",
            " Embedding-Position (PositionEmbedding  (None, None, 768)         393216        ['Embedding-Token-Segment[0][0]']       \n",
            " )                                                                                                                      \n",
            "                                                                                                                        \n",
            " Embedding-Dropout (Dropout)           (None, None, 768)          0             ['Embedding-Position[0][0]']            \n",
            "                                                                                                                        \n",
            " Embedding-Norm (LayerNormalization)   (None, None, 768)          1536          ['Embedding-Dropout[0][0]']             \n",
            "                                                                                                                        \n",
            " Encoder-1-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Embedding-Norm[0][0]']                \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-1-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-1-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-1-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Embedding-Norm[0][0]',                \n",
            " (Add)                                                                           'Encoder-1-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-1-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-1-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-1-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-1-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-1-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-1-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-1-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-1-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-1-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-1-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-1-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-2-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-1-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-2-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-2-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-2-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-1-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-2-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-2-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-2-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-2-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-2-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-2-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-2-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-2-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-2-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-2-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-2-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-2-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-3-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-2-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-3-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-3-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-3-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-2-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-3-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-3-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-3-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-3-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-3-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-3-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-3-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-3-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-3-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-3-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-3-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-3-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-4-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-3-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-4-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-4-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-4-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-3-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-4-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-4-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-4-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-4-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-4-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-4-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-4-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-4-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-4-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-4-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-4-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-4-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-5-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-4-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-5-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-5-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-5-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-4-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-5-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-5-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-5-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-5-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-5-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-5-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-5-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-5-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-5-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-5-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-5-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-5-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-6-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-5-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-6-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-6-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-6-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-5-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-6-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-6-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-6-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-6-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-6-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-6-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-6-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-6-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-6-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-6-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-6-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-6-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-7-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-6-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-7-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-7-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-7-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-6-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-7-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-7-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-7-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-7-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-7-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-7-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-7-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-7-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-7-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-7-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-7-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-7-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-8-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-7-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-8-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-8-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-8-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-7-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-8-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-8-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-8-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-8-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-8-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-8-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-8-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-8-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-8-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-8-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-8-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-8-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-9-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-8-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-9-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-9-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-9-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-8-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-9-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-9-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-9-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-9-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-9-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-9-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-9-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-9-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-9-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-9-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-9-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-9-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-10-MultiHeadSelfAttention (Mu  (None, None, 768)         2362368       ['Encoder-9-FeedForward-Norm[0][0]']    \n",
            " ltiHeadAttention)                                                                                                      \n",
            "                                                                                                                        \n",
            " Encoder-10-MultiHeadSelfAttention-Dro  (None, None, 768)         0             ['Encoder-10-MultiHeadSelfAttention[0][0\n",
            " pout (Dropout)                                                                 ]']                                     \n",
            "                                                                                                                        \n",
            " Encoder-10-MultiHeadSelfAttention-Add  (None, None, 768)         0             ['Encoder-9-FeedForward-Norm[0][0]',    \n",
            "  (Add)                                                                          'Encoder-10-MultiHeadSelfAttention-Drop\n",
            "                                                                                out[0][0]']                             \n",
            "                                                                                                                        \n",
            " Encoder-10-MultiHeadSelfAttention-Nor  (None, None, 768)         1536          ['Encoder-10-MultiHeadSelfAttention-Add[\n",
            " m (LayerNormalization)                                                         0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-10-FeedForward (FeedForward)  (None, None, 768)          4722432       ['Encoder-10-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]']                                \n",
            "                                                                                                                        \n",
            " Encoder-10-FeedForward-Dropout (Dropo  (None, None, 768)         0             ['Encoder-10-FeedForward[0][0]']        \n",
            " ut)                                                                                                                    \n",
            "                                                                                                                        \n",
            " Encoder-10-FeedForward-Add (Add)      (None, None, 768)          0             ['Encoder-10-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]',                                \n",
            "                                                                                 'Encoder-10-FeedForward-Dropout[0][0]']\n",
            "                                                                                                                        \n",
            " Encoder-10-FeedForward-Norm (LayerNor  (None, None, 768)         1536          ['Encoder-10-FeedForward-Add[0][0]']    \n",
            " malization)                                                                                                            \n",
            "                                                                                                                        \n",
            " Encoder-11-MultiHeadSelfAttention (Mu  (None, None, 768)         2362368       ['Encoder-10-FeedForward-Norm[0][0]']   \n",
            " ltiHeadAttention)                                                                                                      \n",
            "                                                                                                                        \n",
            " Encoder-11-MultiHeadSelfAttention-Dro  (None, None, 768)         0             ['Encoder-11-MultiHeadSelfAttention[0][0\n",
            " pout (Dropout)                                                                 ]']                                     \n",
            "                                                                                                                        \n",
            " Encoder-11-MultiHeadSelfAttention-Add  (None, None, 768)         0             ['Encoder-10-FeedForward-Norm[0][0]',   \n",
            "  (Add)                                                                          'Encoder-11-MultiHeadSelfAttention-Drop\n",
            "                                                                                out[0][0]']                             \n",
            "                                                                                                                        \n",
            " Encoder-11-MultiHeadSelfAttention-Nor  (None, None, 768)         1536          ['Encoder-11-MultiHeadSelfAttention-Add[\n",
            " m (LayerNormalization)                                                         0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-11-FeedForward (FeedForward)  (None, None, 768)          4722432       ['Encoder-11-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]']                                \n",
            "                                                                                                                        \n",
            " Encoder-11-FeedForward-Dropout (Dropo  (None, None, 768)         0             ['Encoder-11-FeedForward[0][0]']        \n",
            " ut)                                                                                                                    \n",
            "                                                                                                                        \n",
            " Encoder-11-FeedForward-Add (Add)      (None, None, 768)          0             ['Encoder-11-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]',                                \n",
            "                                                                                 'Encoder-11-FeedForward-Dropout[0][0]']\n",
            "                                                                                                                        \n",
            " Encoder-11-FeedForward-Norm (LayerNor  (None, None, 768)         1536          ['Encoder-11-FeedForward-Add[0][0]']    \n",
            " malization)                                                                                                            \n",
            "                                                                                                                        \n",
            " Encoder-12-MultiHeadSelfAttention (Mu  (None, None, 768)         2362368       ['Encoder-11-FeedForward-Norm[0][0]']   \n",
            " ltiHeadAttention)                                                                                                      \n",
            "                                                                                                                        \n",
            " Encoder-12-MultiHeadSelfAttention-Dro  (None, None, 768)         0             ['Encoder-12-MultiHeadSelfAttention[0][0\n",
            " pout (Dropout)                                                                 ]']                                     \n",
            "                                                                                                                        \n",
            " Encoder-12-MultiHeadSelfAttention-Add  (None, None, 768)         0             ['Encoder-11-FeedForward-Norm[0][0]',   \n",
            "  (Add)                                                                          'Encoder-12-MultiHeadSelfAttention-Drop\n",
            "                                                                                out[0][0]']                             \n",
            "                                                                                                                        \n",
            " Encoder-12-MultiHeadSelfAttention-Nor  (None, None, 768)         1536          ['Encoder-12-MultiHeadSelfAttention-Add[\n",
            " m (LayerNormalization)                                                         0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-12-FeedForward (FeedForward)  (None, None, 768)          4722432       ['Encoder-12-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]']                                \n",
            "                                                                                                                        \n",
            " Encoder-12-FeedForward-Dropout (Dropo  (None, None, 768)         0             ['Encoder-12-FeedForward[0][0]']        \n",
            " ut)                                                                                                                    \n",
            "                                                                                                                        \n",
            " Encoder-12-FeedForward-Add (Add)      (None, None, 768)          0             ['Encoder-12-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]',                                \n",
            "                                                                                 'Encoder-12-FeedForward-Dropout[0][0]']\n",
            "                                                                                                                        \n",
            " Encoder-12-FeedForward-Norm (LayerNor  (None, None, 768)         1536          ['Encoder-12-FeedForward-Add[0][0]']    \n",
            " malization)                                                                                                            \n",
            "                                                                                                                        \n",
            "========================================================================================================================\n",
            "Total params: 101,677,056\n",
            "Trainable params: 0\n",
            "Non-trainable params: 101,677,056\n",
            "________________________________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_path = get_pretrained(PretrainedList.chinese_base)\n",
        "paths = get_checkpoint_paths(model_path)\n",
        "bert_model = load_trained_model_from_checkpoint(\n",
        "    paths.config, paths.checkpoint, training=False, seq_len=None\n",
        ")\n",
        "bert_model.summary(line_length=120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EMQNIppDX6lt"
      },
      "outputs": [],
      "source": [
        "token_dict = load_vocabulary(paths.vocab)\n",
        "len(token_dict)\n",
        "tokenizer = Tokenizer(token_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw9v_j6FdNBP"
      },
      "source": [
        "載入打散後資料"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lI3L2ujxdHmY"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('./data/r_text.pickle', 'rb') as f:\n",
        "    x_text = pickle.load(f)\n",
        "with open('./data/r_tag.pickle', 'rb') as f:\n",
        "    y_tag = pickle.load(f)\n",
        "\n",
        "X = x_text[:]\n",
        "Y = y_tag[:]\n",
        "#len(x_text)=6317"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "a5a2KyoNfR9o"
      },
      "outputs": [],
      "source": [
        "#設置模型\n",
        "def build_model():\n",
        "    # 把 BERT 模型裡的每一層的trainable屬性都設成True，讓BERT模型裡的權重都可以被訓練\n",
        "    for l in bert_model.layers:\n",
        "        l.trainable = True\n",
        "\n",
        "    # 模型的輸入，我們會需要兩個輸入，分別是語句內容以及其對應的segments\n",
        "    x1_in = Input(shape=(None,))\n",
        "    x2_in = Input(shape=(None,))\n",
        "    x = bert_model([x1_in, x2_in])\n",
        "    x = Lambda(lambda x: x[:, 0])(x) # 取出[CLS]對應位置的向量出來\n",
        "    p = Dense(18, activation='softmax')(x) # 輸出情緒傾向數值\n",
        "\n",
        "    model = Model([x1_in, x2_in], p)\n",
        "\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=Adam(1e-5),    #用較小的學習率\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    #model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AtZ4KPqMaWya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ff2a58e-75b7-4a85-ef46-7a5bde18553e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing fold # 0\n",
            "Epoch 1/5\n",
            "569/569 [==============================] - 279s 456ms/step - loss: 1.0321 - accuracy: 0.7019\n",
            "Epoch 2/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 0.3245 - accuracy: 0.9078\n",
            "Epoch 3/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 0.1490 - accuracy: 0.9588\n",
            "Epoch 4/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 0.0619 - accuracy: 0.9847\n",
            "Epoch 5/5\n",
            "569/569 [==============================] - 259s 455ms/step - loss: 0.0350 - accuracy: 0.9910\n",
            "64/64 [==============================] - 14s 154ms/step - loss: 0.4185 - accuracy: 0.9081\n",
            "processing fold # 1\n",
            "Epoch 1/5\n",
            "569/569 [==============================] - 277s 455ms/step - loss: 0.3035 - accuracy: 0.9173\n",
            "Epoch 2/5\n",
            "569/569 [==============================] - 259s 455ms/step - loss: 0.0528 - accuracy: 0.9873\n",
            "Epoch 3/5\n",
            "569/569 [==============================] - 259s 455ms/step - loss: 0.0358 - accuracy: 0.9909\n",
            "Epoch 4/5\n",
            "569/569 [==============================] - 259s 455ms/step - loss: 0.0277 - accuracy: 0.9923\n",
            "Epoch 5/5\n",
            "569/569 [==============================] - 259s 455ms/step - loss: 0.0159 - accuracy: 0.9963\n",
            "64/64 [==============================] - 13s 153ms/step - loss: 0.0854 - accuracy: 0.9715\n",
            "processing fold # 2\n",
            "Epoch 1/5\n",
            "569/569 [==============================] - 277s 456ms/step - loss: 0.1649 - accuracy: 0.9610\n",
            "Epoch 2/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 0.0166 - accuracy: 0.9963\n",
            "Epoch 3/5\n",
            "569/569 [==============================] - 260s 456ms/step - loss: 0.0268 - accuracy: 0.9937\n",
            "Epoch 4/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 0.0293 - accuracy: 0.9919\n",
            "Epoch 5/5\n",
            "569/569 [==============================] - 260s 456ms/step - loss: 0.0211 - accuracy: 0.9935\n",
            "64/64 [==============================] - 14s 154ms/step - loss: 0.0731 - accuracy: 0.9842\n",
            "processing fold # 3\n",
            "Epoch 1/5\n",
            "569/569 [==============================] - 277s 456ms/step - loss: 0.1454 - accuracy: 0.9655\n",
            "Epoch 2/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 0.0190 - accuracy: 0.9945\n",
            "Epoch 3/5\n",
            "569/569 [==============================] - 260s 456ms/step - loss: 0.0076 - accuracy: 0.9984\n",
            "Epoch 4/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 0.0091 - accuracy: 0.9979\n",
            "Epoch 5/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 0.0425 - accuracy: 0.9882\n",
            "64/64 [==============================] - 14s 153ms/step - loss: 0.0422 - accuracy: 0.9889\n",
            "processing fold # 4\n",
            "Epoch 1/5\n",
            "569/569 [==============================] - 278s 456ms/step - loss: 0.1162 - accuracy: 0.9756\n",
            "Epoch 2/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 0.0100 - accuracy: 0.9977\n",
            "Epoch 3/5\n",
            "569/569 [==============================] - 260s 456ms/step - loss: 0.0148 - accuracy: 0.9961\n",
            "Epoch 4/5\n",
            "569/569 [==============================] - 260s 456ms/step - loss: 0.0175 - accuracy: 0.9956\n",
            "Epoch 5/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 0.0203 - accuracy: 0.9947\n",
            "64/64 [==============================] - 13s 153ms/step - loss: 0.0076 - accuracy: 0.9984\n",
            "processing fold # 5\n",
            "Epoch 1/5\n",
            "569/569 [==============================] - 278s 456ms/step - loss: 0.1221 - accuracy: 0.9738\n",
            "Epoch 2/5\n",
            "569/569 [==============================] - 260s 456ms/step - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "569/569 [==============================] - 260s 456ms/step - loss: 0.0082 - accuracy: 0.9979\n",
            "Epoch 4/5\n",
            "569/569 [==============================] - 260s 456ms/step - loss: 0.0263 - accuracy: 0.9916\n",
            "Epoch 5/5\n",
            "569/569 [==============================] - 260s 456ms/step - loss: 0.0222 - accuracy: 0.9942\n",
            "64/64 [==============================] - 14s 153ms/step - loss: 0.0128 - accuracy: 0.9952\n",
            "processing fold # 6\n",
            "Epoch 1/5\n",
            "569/569 [==============================] - 277s 455ms/step - loss: 0.1057 - accuracy: 0.9770\n",
            "Epoch 2/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 0.0110 - accuracy: 0.9974\n",
            "Epoch 3/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 0.0063 - accuracy: 0.9993\n",
            "Epoch 4/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 0.0136 - accuracy: 0.9974\n",
            "Epoch 5/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 0.0177 - accuracy: 0.9956\n",
            "64/64 [==============================] - 15s 153ms/step - loss: 0.0293 - accuracy: 0.9952\n",
            "processing fold # 7\n",
            "Epoch 1/5\n",
            "569/569 [==============================] - 279s 456ms/step - loss: 0.1154 - accuracy: 0.9747\n",
            "Epoch 2/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 0.0026 - accuracy: 0.9996\n",
            "Epoch 3/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 5.8969e-04 - accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 3.3614e-04 - accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "569/569 [==============================] - 260s 456ms/step - loss: 1.9583e-04 - accuracy: 1.0000\n",
            "64/64 [==============================] - 14s 154ms/step - loss: 7.5621e-05 - accuracy: 1.0000\n",
            "processing fold # 8\n",
            "Epoch 1/5\n",
            "569/569 [==============================] - 279s 456ms/step - loss: 0.0799 - accuracy: 0.9842\n",
            "Epoch 2/5\n",
            "569/569 [==============================] - 260s 456ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "569/569 [==============================] - 260s 456ms/step - loss: 4.3201e-04 - accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "569/569 [==============================] - 260s 456ms/step - loss: 2.4868e-04 - accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 1.4768e-04 - accuracy: 1.0000\n",
            "64/64 [==============================] - 14s 153ms/step - loss: 8.5947e-05 - accuracy: 1.0000\n",
            "processing fold # 9\n",
            "Epoch 1/5\n",
            "569/569 [==============================] - 280s 456ms/step - loss: 0.0944 - accuracy: 0.9796\n",
            "Epoch 2/5\n",
            "569/569 [==============================] - 260s 456ms/step - loss: 0.0090 - accuracy: 0.9977\n",
            "Epoch 3/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 0.0093 - accuracy: 0.9975\n",
            "Epoch 4/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 0.0178 - accuracy: 0.9961\n",
            "Epoch 5/5\n",
            "569/569 [==============================] - 259s 456ms/step - loss: 0.0025 - accuracy: 0.9995\n",
            "64/64 [==============================] - 14s 153ms/step - loss: 1.9465e-04 - accuracy: 1.0000\n",
            "98.42% (+/- 2.68%)\n",
            "End of Predict DataSet!\n"
          ]
        }
      ],
      "source": [
        "k=10\n",
        "num_val_samples = len(X) // k\n",
        "cvscores = []\n",
        "results = []\n",
        "for i in range(k):\n",
        "    print('processing fold #',i)\n",
        "    test_data = X[i*num_val_samples:(i+1)*num_val_samples]\n",
        "    tag_test = Y[i*num_val_samples:(i+1)*num_val_samples]\n",
        "    train_data = np.concatenate(\n",
        "        [X[:i*num_val_samples],X[(i+1)*num_val_samples:]],axis=0)\n",
        "    tag_train = np.concatenate(\n",
        "        [Y[:i*num_val_samples],Y[(i+1)*num_val_samples:]],axis=0)\n",
        "\n",
        "    partial_train_data = []\n",
        "    partial_train_targets = []\n",
        "    val_data = []\n",
        "    val_targets = []\n",
        "\n",
        "    indices = []\n",
        "    for sentence in train_data:\n",
        "        ids, segments = tokenizer.encode(sentence, max_len=300) # Tokenizer進行分詞\n",
        "        indices.append(ids)\n",
        "    indices = np.array(indices)\n",
        "    partial_train_data = [indices, np.zeros_like(indices)]\n",
        "    partial_train_targets = np.array(tag_train)\n",
        "\n",
        "    indices = []\n",
        "    for sentence in test_data:\n",
        "        ids, segments = tokenizer.encode(sentence, max_len=300) # Tokenizer進行分詞\n",
        "        indices.append(ids)\n",
        "    indices = np.array(indices)\n",
        "    val_data = [indices, np.zeros_like(indices)]\n",
        "    val_targets = np.array(tag_test)\n",
        "    \n",
        "    model = build_model()   \n",
        "    \n",
        "    history = model.fit(\n",
        "        partial_train_data,partial_train_targets,\n",
        "        batch_size=BATCH_SIZE,epochs=EPOCHS,verbose=1)\n",
        "# evaluate the model\n",
        "    scores = model.evaluate(val_data,val_targets,batch_size=BATCH_SIZE, verbose=1) \n",
        "    result = model.predict(val_data)\n",
        "    results.append(result)\n",
        "    cvscores.append(scores[1] * 100)\n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
        "print(\"End of Predict DataSet!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zjNAlnvxagDV"
      },
      "outputs": [],
      "source": [
        "model.save('bert-k10.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RecxoCJbarla"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('bertk10_results.pickle', 'wb') as f:\n",
        "    pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    with open('/trainHistoryDict', 'wb') as file_pi:\n",
        "        pickle.dump(history.history, file_pi)"
      ],
      "metadata": {
        "id": "7aICD2H2LmQ-"
      },
      "execution_count": 15,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "MyBert_k10.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}