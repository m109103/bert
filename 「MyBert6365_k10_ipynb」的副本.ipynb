{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m109103/bert/blob/main/%E3%80%8CMyBert6365_k10_ipynb%E3%80%8D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1lnuesyxCqE",
        "outputId": "d77e5434-c7e4-4a33-c7cf-ada9f5c108cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-bert\n",
            "  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-bert) (1.21.6)\n",
            "Collecting keras-transformer==0.40.0\n",
            "  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
            "Collecting keras-pos-embd==0.13.0\n",
            "  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
            "Collecting keras-multi-head==0.29.0\n",
            "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
            "Collecting keras-layer-normalization==0.16.0\n",
            "  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
            "Collecting keras-position-wise-feed-forward==0.8.0\n",
            "  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
            "Collecting keras-embed-sim==0.10.0\n",
            "  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
            "Collecting keras-self-attention==0.51.0\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33517 sha256=1edd26146d2e0836a5843ac7e2543a1f2b2f5b625216debde713ee7b4f0904e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/e8/45/842b3a39831261aef9154b907eacbc4ac99499a99ae829b06f\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12305 sha256=298f65c1869d7defaac8dc7a5f3f15dddd7c6abe849a25402bf7fa78c82306e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/68/26/692ed21edd832833c3b0a0e21615bcacd99ca458b3f9ed571f\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3960 sha256=5e9af93d1aae23fd8cb34ac0f85893c8b09382757ac7867f7598c9b5d281b600\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/67/b5/d847588d075895281e1cf5590f819bd4cf076a554872268bd5\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4668 sha256=b6809257e0ef5cef9ee07d3c85117d07abb6bd0cc14860f9a54d98e6622519a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/5d/1c/2e619f594f69fbcf8bc20943b27d414871c409be053994813e\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14993 sha256=adf67e0ea1cfaca04ecdd90fd0a9b9c0069504d5099e082e7efa749e6fccc148\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/aa/3c/9d15d24005179dae08ff291ce99c754b296347817d076fd9fb\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6962 sha256=16db340abf457c43661adff1d7608267fe4d3c7605aea7fd6852fc0ac2c9d630\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/c1/a0/dc44fcf68c857b7ff6be9a97e675e5adf51022eff1169b042f\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4983 sha256=39ca8b26d286cdeccb4660dc71388340175c5fd44845d73c0b8c63353429080a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/75/6f/d42f6e051506f442daeba53ff1e2d21a5f20ef8c411610f2bb\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=8220407d5b509486c21d5c43afcfff7e044ca3fc280e9221b2de9725a1f6a821\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
            "Successfully built keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention\n",
            "Installing collected packages: keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.89.0 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCTpza-0LRzB",
        "outputId": "d8ae916a-6c7d-479c-a591-0a5a856594e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y0Vg7WwLkZN",
        "outputId": "6f91289d-1752-4eb7-a144-6bc4b3e42e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "1.6512027820000128\n",
            "GPU (s):\n",
            "0.04010809400000426\n",
            "GPU speedup over CPU: 41x\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQChBWn7C2oP",
        "outputId": "3f504f91-9fbd-4484-c2ff-6d2dc25d1f75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/Colab Notebooks/bert6365\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive  \n",
        "drive.mount('/content/gdrive')  \n",
        "%cd /content/gdrive/\"My Drive\"/\"Colab Notebooks\"/\"bert6365\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZOcUU4i0IsC"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from keras_bert import (\n",
        "    load_vocabulary,\n",
        "    load_trained_model_from_checkpoint,\n",
        "    Tokenizer,\n",
        "    get_checkpoint_paths,\n",
        ")\n",
        "from keras_bert.datasets import get_pretrained, PretrainedList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujxw-JrC0OHS"
      },
      "outputs": [],
      "source": [
        "# 指定訓練資料與測試資料的比例\n",
        "BATCH_SIZE = 10 # batch size建議不要設得太大，不然很有可能out of memory\n",
        "EPOCHS = 5 # epoch 5次其實就很夠了，當然你可以嘗試再大一點，只是訓練要更久"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW01f8ZM0ZyT",
        "outputId": "21ffbce7-ab9b-4dcb-da48-87b99ee00c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n",
            "381894656/381892918 [==============================] - 3s 0us/step\n",
            "381902848/381892918 [==============================] - 3s 0us/step\n",
            "Model: \"model_1\"\n",
            "________________________________________________________________________________________________________________________\n",
            " Layer (type)                          Output Shape               Param #       Connected to                            \n",
            "========================================================================================================================\n",
            " Input-Token (InputLayer)              [(None, None)]             0             []                                      \n",
            "                                                                                                                        \n",
            " Input-Segment (InputLayer)            [(None, None)]             0             []                                      \n",
            "                                                                                                                        \n",
            " Embedding-Token (TokenEmbedding)      [(None, None, 768),        16226304      ['Input-Token[0][0]']                   \n",
            "                                        (21128, 768)]                                                                   \n",
            "                                                                                                                        \n",
            " Embedding-Segment (Embedding)         (None, None, 768)          1536          ['Input-Segment[0][0]']                 \n",
            "                                                                                                                        \n",
            " Embedding-Token-Segment (Add)         (None, None, 768)          0             ['Embedding-Token[0][0]',               \n",
            "                                                                                 'Embedding-Segment[0][0]']             \n",
            "                                                                                                                        \n",
            " Embedding-Position (PositionEmbedding  (None, None, 768)         393216        ['Embedding-Token-Segment[0][0]']       \n",
            " )                                                                                                                      \n",
            "                                                                                                                        \n",
            " Embedding-Dropout (Dropout)           (None, None, 768)          0             ['Embedding-Position[0][0]']            \n",
            "                                                                                                                        \n",
            " Embedding-Norm (LayerNormalization)   (None, None, 768)          1536          ['Embedding-Dropout[0][0]']             \n",
            "                                                                                                                        \n",
            " Encoder-1-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Embedding-Norm[0][0]']                \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-1-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-1-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-1-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Embedding-Norm[0][0]',                \n",
            " (Add)                                                                           'Encoder-1-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-1-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-1-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-1-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-1-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-1-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-1-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-1-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-1-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-1-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-1-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-1-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-2-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-1-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-2-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-2-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-2-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-1-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-2-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-2-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-2-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-2-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-2-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-2-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-2-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-2-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-2-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-2-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-2-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-2-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-3-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-2-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-3-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-3-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-3-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-2-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-3-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-3-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-3-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-3-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-3-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-3-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-3-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-3-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-3-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-3-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-3-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-3-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-4-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-3-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-4-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-4-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-4-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-3-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-4-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-4-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-4-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-4-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-4-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-4-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-4-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-4-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-4-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-4-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-4-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-4-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-5-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-4-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-5-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-5-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-5-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-4-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-5-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-5-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-5-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-5-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-5-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-5-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-5-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-5-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-5-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-5-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-5-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-5-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-6-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-5-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-6-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-6-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-6-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-5-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-6-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-6-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-6-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-6-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-6-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-6-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-6-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-6-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-6-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-6-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-6-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-6-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-7-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-6-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-7-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-7-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-7-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-6-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-7-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-7-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-7-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-7-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-7-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-7-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-7-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-7-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-7-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-7-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-7-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-7-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-8-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-7-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-8-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-8-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-8-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-7-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-8-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-8-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-8-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-8-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-8-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-8-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-8-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-8-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-8-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-8-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-8-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-8-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-9-MultiHeadSelfAttention (Mul  (None, None, 768)         2362368       ['Encoder-8-FeedForward-Norm[0][0]']    \n",
            " tiHeadAttention)                                                                                                       \n",
            "                                                                                                                        \n",
            " Encoder-9-MultiHeadSelfAttention-Drop  (None, None, 768)         0             ['Encoder-9-MultiHeadSelfAttention[0][0]\n",
            " out (Dropout)                                                                  ']                                      \n",
            "                                                                                                                        \n",
            " Encoder-9-MultiHeadSelfAttention-Add   (None, None, 768)         0             ['Encoder-8-FeedForward-Norm[0][0]',    \n",
            " (Add)                                                                           'Encoder-9-MultiHeadSelfAttention-Dropo\n",
            "                                                                                ut[0][0]']                              \n",
            "                                                                                                                        \n",
            " Encoder-9-MultiHeadSelfAttention-Norm  (None, None, 768)         1536          ['Encoder-9-MultiHeadSelfAttention-Add[0\n",
            "  (LayerNormalization)                                                          ][0]']                                  \n",
            "                                                                                                                        \n",
            " Encoder-9-FeedForward (FeedForward)   (None, None, 768)          4722432       ['Encoder-9-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-9-FeedForward-Dropout (Dropou  (None, None, 768)         0             ['Encoder-9-FeedForward[0][0]']         \n",
            " t)                                                                                                                     \n",
            "                                                                                                                        \n",
            " Encoder-9-FeedForward-Add (Add)       (None, None, 768)          0             ['Encoder-9-MultiHeadSelfAttention-Norm[\n",
            "                                                                                0][0]',                                 \n",
            "                                                                                 'Encoder-9-FeedForward-Dropout[0][0]'] \n",
            "                                                                                                                        \n",
            " Encoder-9-FeedForward-Norm (LayerNorm  (None, None, 768)         1536          ['Encoder-9-FeedForward-Add[0][0]']     \n",
            " alization)                                                                                                             \n",
            "                                                                                                                        \n",
            " Encoder-10-MultiHeadSelfAttention (Mu  (None, None, 768)         2362368       ['Encoder-9-FeedForward-Norm[0][0]']    \n",
            " ltiHeadAttention)                                                                                                      \n",
            "                                                                                                                        \n",
            " Encoder-10-MultiHeadSelfAttention-Dro  (None, None, 768)         0             ['Encoder-10-MultiHeadSelfAttention[0][0\n",
            " pout (Dropout)                                                                 ]']                                     \n",
            "                                                                                                                        \n",
            " Encoder-10-MultiHeadSelfAttention-Add  (None, None, 768)         0             ['Encoder-9-FeedForward-Norm[0][0]',    \n",
            "  (Add)                                                                          'Encoder-10-MultiHeadSelfAttention-Drop\n",
            "                                                                                out[0][0]']                             \n",
            "                                                                                                                        \n",
            " Encoder-10-MultiHeadSelfAttention-Nor  (None, None, 768)         1536          ['Encoder-10-MultiHeadSelfAttention-Add[\n",
            " m (LayerNormalization)                                                         0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-10-FeedForward (FeedForward)  (None, None, 768)          4722432       ['Encoder-10-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]']                                \n",
            "                                                                                                                        \n",
            " Encoder-10-FeedForward-Dropout (Dropo  (None, None, 768)         0             ['Encoder-10-FeedForward[0][0]']        \n",
            " ut)                                                                                                                    \n",
            "                                                                                                                        \n",
            " Encoder-10-FeedForward-Add (Add)      (None, None, 768)          0             ['Encoder-10-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]',                                \n",
            "                                                                                 'Encoder-10-FeedForward-Dropout[0][0]']\n",
            "                                                                                                                        \n",
            " Encoder-10-FeedForward-Norm (LayerNor  (None, None, 768)         1536          ['Encoder-10-FeedForward-Add[0][0]']    \n",
            " malization)                                                                                                            \n",
            "                                                                                                                        \n",
            " Encoder-11-MultiHeadSelfAttention (Mu  (None, None, 768)         2362368       ['Encoder-10-FeedForward-Norm[0][0]']   \n",
            " ltiHeadAttention)                                                                                                      \n",
            "                                                                                                                        \n",
            " Encoder-11-MultiHeadSelfAttention-Dro  (None, None, 768)         0             ['Encoder-11-MultiHeadSelfAttention[0][0\n",
            " pout (Dropout)                                                                 ]']                                     \n",
            "                                                                                                                        \n",
            " Encoder-11-MultiHeadSelfAttention-Add  (None, None, 768)         0             ['Encoder-10-FeedForward-Norm[0][0]',   \n",
            "  (Add)                                                                          'Encoder-11-MultiHeadSelfAttention-Drop\n",
            "                                                                                out[0][0]']                             \n",
            "                                                                                                                        \n",
            " Encoder-11-MultiHeadSelfAttention-Nor  (None, None, 768)         1536          ['Encoder-11-MultiHeadSelfAttention-Add[\n",
            " m (LayerNormalization)                                                         0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-11-FeedForward (FeedForward)  (None, None, 768)          4722432       ['Encoder-11-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]']                                \n",
            "                                                                                                                        \n",
            " Encoder-11-FeedForward-Dropout (Dropo  (None, None, 768)         0             ['Encoder-11-FeedForward[0][0]']        \n",
            " ut)                                                                                                                    \n",
            "                                                                                                                        \n",
            " Encoder-11-FeedForward-Add (Add)      (None, None, 768)          0             ['Encoder-11-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]',                                \n",
            "                                                                                 'Encoder-11-FeedForward-Dropout[0][0]']\n",
            "                                                                                                                        \n",
            " Encoder-11-FeedForward-Norm (LayerNor  (None, None, 768)         1536          ['Encoder-11-FeedForward-Add[0][0]']    \n",
            " malization)                                                                                                            \n",
            "                                                                                                                        \n",
            " Encoder-12-MultiHeadSelfAttention (Mu  (None, None, 768)         2362368       ['Encoder-11-FeedForward-Norm[0][0]']   \n",
            " ltiHeadAttention)                                                                                                      \n",
            "                                                                                                                        \n",
            " Encoder-12-MultiHeadSelfAttention-Dro  (None, None, 768)         0             ['Encoder-12-MultiHeadSelfAttention[0][0\n",
            " pout (Dropout)                                                                 ]']                                     \n",
            "                                                                                                                        \n",
            " Encoder-12-MultiHeadSelfAttention-Add  (None, None, 768)         0             ['Encoder-11-FeedForward-Norm[0][0]',   \n",
            "  (Add)                                                                          'Encoder-12-MultiHeadSelfAttention-Drop\n",
            "                                                                                out[0][0]']                             \n",
            "                                                                                                                        \n",
            " Encoder-12-MultiHeadSelfAttention-Nor  (None, None, 768)         1536          ['Encoder-12-MultiHeadSelfAttention-Add[\n",
            " m (LayerNormalization)                                                         0][0]']                                 \n",
            "                                                                                                                        \n",
            " Encoder-12-FeedForward (FeedForward)  (None, None, 768)          4722432       ['Encoder-12-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]']                                \n",
            "                                                                                                                        \n",
            " Encoder-12-FeedForward-Dropout (Dropo  (None, None, 768)         0             ['Encoder-12-FeedForward[0][0]']        \n",
            " ut)                                                                                                                    \n",
            "                                                                                                                        \n",
            " Encoder-12-FeedForward-Add (Add)      (None, None, 768)          0             ['Encoder-12-MultiHeadSelfAttention-Norm\n",
            "                                                                                [0][0]',                                \n",
            "                                                                                 'Encoder-12-FeedForward-Dropout[0][0]']\n",
            "                                                                                                                        \n",
            " Encoder-12-FeedForward-Norm (LayerNor  (None, None, 768)         1536          ['Encoder-12-FeedForward-Add[0][0]']    \n",
            " malization)                                                                                                            \n",
            "                                                                                                                        \n",
            "========================================================================================================================\n",
            "Total params: 101,677,056\n",
            "Trainable params: 0\n",
            "Non-trainable params: 101,677,056\n",
            "________________________________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_path = get_pretrained(PretrainedList.chinese_base)\n",
        "paths = get_checkpoint_paths(model_path)\n",
        "bert_model = load_trained_model_from_checkpoint(\n",
        "    paths.config, paths.checkpoint, training=False, seq_len=None\n",
        ")\n",
        "bert_model.summary(line_length=120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMQNIppDX6lt"
      },
      "outputs": [],
      "source": [
        "token_dict = load_vocabulary(paths.vocab)\n",
        "len(token_dict)\n",
        "tokenizer = Tokenizer(token_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw9v_j6FdNBP"
      },
      "source": [
        "載入打散後資料"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI3L2ujxdHmY"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('x_train.pickle', 'rb') as f:\n",
        "    x_text = pickle.load(f)\n",
        "with open('y_train.pickle', 'rb') as f:\n",
        "    y_tag = pickle.load(f)\n",
        "\n",
        "X = x_text[:]\n",
        "Y = y_tag[:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5a2KyoNfR9o"
      },
      "outputs": [],
      "source": [
        "#設置模型\n",
        "def build_model():\n",
        "    #bert_model.summary(line_length=120)\n",
        "    model_path = get_pretrained(PretrainedList.chinese_base)\n",
        "    paths = get_checkpoint_paths(model_path)\n",
        "    bert_model = load_trained_model_from_checkpoint(\n",
        "     paths.config, paths.checkpoint, training=False, seq_len=None\n",
        "    )\n",
        "    #bert Tokenizer\n",
        "    token_dict = load_vocabulary(paths.vocab)\n",
        "    len(token_dict)\n",
        "    tokenizer = Tokenizer(token_dict)\n",
        "    # 把 BERT 模型裡的每一層的trainable屬性都設成True，讓BERT模型裡的權重都可以被訓練\n",
        "    for l in bert_model.layers:\n",
        "        l.trainable = True\n",
        "\n",
        "    # 模型的輸入，我們會需要兩個輸入，分別是語句內容以及其對應的segments\n",
        "    x1_in = Input(shape=(None,))\n",
        "    x2_in = Input(shape=(None,))\n",
        "    x = bert_model([x1_in, x2_in])\n",
        "    x = Lambda(lambda x: x[:, 0])(x) # 取出[CLS]對應位置的向量出來\n",
        "    p = Dense(18, activation='softmax')(x) # 輸出情緒傾向數值\n",
        "\n",
        "    model = Model([x1_in, x2_in], p)\n",
        "\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=Adam(1e-5),    #用較小的學習率\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    #model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtZ4KPqMaWya",
        "outputId": "29c56994-fb0f-4e21-ce7e-84159b63e71f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing fold # 0\n",
            "64/64 [==============================] - 14s 156ms/step - loss: 0.4093 - accuracy: 0.9167\n",
            "processing fold # 1\n",
            "64/64 [==============================] - 14s 155ms/step - loss: 0.2968 - accuracy: 0.9261\n",
            "processing fold # 2\n",
            "64/64 [==============================] - 15s 155ms/step - loss: 0.3589 - accuracy: 0.9198\n",
            "processing fold # 3\n",
            "64/64 [==============================] - 14s 155ms/step - loss: 0.3572 - accuracy: 0.9245\n",
            "processing fold # 4\n",
            "64/64 [==============================] - 15s 155ms/step - loss: 0.4734 - accuracy: 0.9009\n",
            "processing fold # 5\n",
            "64/64 [==============================] - 14s 155ms/step - loss: 0.3399 - accuracy: 0.9119\n",
            "processing fold # 6\n",
            "64/64 [==============================] - 15s 156ms/step - loss: 0.3720 - accuracy: 0.9182\n",
            "processing fold # 7\n",
            "64/64 [==============================] - 15s 157ms/step - loss: 0.4144 - accuracy: 0.9041\n",
            "processing fold # 8\n",
            "64/64 [==============================] - 16s 158ms/step - loss: 0.4051 - accuracy: 0.9057\n",
            "processing fold # 9\n",
            "64/64 [==============================] - 15s 157ms/step - loss: 0.3518 - accuracy: 0.9198\n",
            "91.48% (+/- 0.83%)\n",
            "End of Predict DataSet!\n"
          ]
        }
      ],
      "source": [
        "k=10\n",
        "num_val_samples = len(X) // k\n",
        "cvscores = []\n",
        "results = []\n",
        "for i in range(k):\n",
        "    print('processing fold #',i)\n",
        "    test_data = X[i*num_val_samples:(i+1)*num_val_samples]\n",
        "    tag_test = Y[i*num_val_samples:(i+1)*num_val_samples]\n",
        "    train_data = np.concatenate(\n",
        "        [X[:i*num_val_samples],X[(i+1)*num_val_samples:]],axis=0)\n",
        "    tag_train = np.concatenate(\n",
        "        [Y[:i*num_val_samples],Y[(i+1)*num_val_samples:]],axis=0)\n",
        "\n",
        "    partial_train_data = []\n",
        "    partial_train_targets = []\n",
        "    val_data = []\n",
        "    val_targets = []\n",
        "\n",
        "    indices = []\n",
        "    for sentence in train_data:\n",
        "        ids, segments = tokenizer.encode(sentence, max_len=300) # Tokenizer進行分詞\n",
        "        indices.append(ids)\n",
        "    indices = np.array(indices)\n",
        "    partial_train_data = [indices, np.zeros_like(indices)]\n",
        "    partial_train_targets = np.array(tag_train)\n",
        "\n",
        "    indices = []\n",
        "    for sentence in test_data:\n",
        "        ids, segments = tokenizer.encode(sentence, max_len=300) # Tokenizer進行分詞\n",
        "        indices.append(ids)\n",
        "    indices = np.array(indices)\n",
        "    val_data = [indices, np.zeros_like(indices)]\n",
        "    val_targets = np.array(tag_test)\n",
        "    \n",
        "    model = build_model()   \n",
        "    \n",
        "    history = model.fit(\n",
        "        partial_train_data,partial_train_targets,\n",
        "        batch_size=BATCH_SIZE,epochs=EPOCHS,verbose=0)\n",
        "# evaluate the model\n",
        "    scores = model.evaluate(val_data,val_targets,batch_size=BATCH_SIZE, verbose=1) \n",
        "    result = model.predict(val_data)\n",
        "    results.append(result)\n",
        "    cvscores.append(scores[1] * 100)\n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
        "print(\"End of Predict DataSet!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjNAlnvxagDV"
      },
      "outputs": [],
      "source": [
        "model.save('bert-k10_6310.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RecxoCJbarla"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('bert6310_k10_results.pickle', 'wb') as f:\n",
        "    pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVk79gcxhLeQ"
      },
      "outputs": [],
      "source": [
        "# convert the history.history dict to a pandas DataFrame:  \n",
        "import pandas as pd   \n",
        "hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "# save to json:  \n",
        "hist_json_file = 'history_k10.json' \n",
        "with open(hist_json_file, mode='w') as f:\n",
        "    hist_df.to_json(f)\n",
        "\n",
        "# or save to csv: \n",
        "hist_csv_file = 'history_k10.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "「MyBert6365_k10.ipynb」的副本",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}